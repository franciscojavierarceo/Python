{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "472e0cdc-5fc8-436a-a2fc-4e6fe272df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/sahanavejlr/evaluating-ir-on-kaggle-dataset\n",
    "# https://www.kaggle.com/datasets/dmaso01dsta/cisi-a-dataset-for-information-retrieval/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6509d30a-7a8a-4370-bc53-5660e1d8af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab1fd9b-f54f-4993-9995-a25b3e76b4c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Using cached jiter-0.5.0-cp310-cp310-macosx_11_0_arm64.whl (299 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting pydantic-core==2.20.1\n",
      "  Using cached pydantic_core-2.20.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: tqdm, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.5.0 openai-1.43.0 pydantic-2.8.2 pydantic-core-2.20.1 tqdm-4.66.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.7.24-cp310-cp310-macosx_11_0_arm64.whl (278 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-2.1.1-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-macosx_11_0_arm64.whl (381 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Installing collected packages: safetensors, regex, numpy, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.9.0 huggingface-hub-0.24.6 numpy-2.1.1 regex-2024.7.24 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.1-py3-none-any.whl (324 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.3/324.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./venv/lib/python3.10/site-packages (from accelerate) (0.24.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from accelerate) (24.1)\n",
      "Collecting torch>=1.10.0\n",
      "  Downloading torch-2.4.1-cp310-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.10/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: pyyaml in ./venv/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./venv/lib/python3.10/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch, accelerate\n",
      "Successfully installed accelerate-0.34.1 mpmath-1.3.0 networkx-3.3 sympy-1.13.2 torch-2.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./venv/lib/python3.10/site-packages (from evaluate) (0.24.6)\n",
      "Collecting datasets>=2.0.0\n",
      "  Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./venv/lib/python3.10/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from evaluate) (24.1)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp310-cp310-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from evaluate) (2.1.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./venv/lib/python3.10/site-packages (from evaluate) (2024.9.0)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.10.5-cp310-cp310-macosx_11_0_arm64.whl (389 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.1/389.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Using cached pyarrow-17.0.0-cp310-cp310-macosx_11_0_arm64.whl (27.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.11-cp310-cp310-macosx_11_0_arm64.whl (111 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.21.0 dill-0.3.8 evaluate-0.4.2 frozenlist-1.4.1 fsspec-2024.6.1 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-17.0.0 pytz-2024.1 tzdata-2024.1 xxhash-3.5.0 yarl-1.9.11\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.1-cp310-cp310-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./venv/lib/python3.10/site-packages (from scikit-learn) (2.1.1)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.14.1-cp310-cp310-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.14.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in ./venv/lib/python3.10/site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./venv/lib/python3.10/site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.10/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.10/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.11)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install openai\n",
    "! pip install transformers\n",
    "! pip install accelerate\n",
    "! pip install evaluate\n",
    "! pip install scikit-learn\n",
    "! pip install sentencepiece\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc21426-41a3-469d-bce8-382b00a2cff4",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "A file of 1,460 \"documents\" each with a unique ID (.I), title (.T), author (.A), abstract (.W) and list of cross-references to other documents (.X). It is the dataset for training IR models when used in conjunction with the Queries (CISI.QRY).\n",
    "\n",
    "\n",
    "## About Dataset\n",
    "\n",
    "### Content\n",
    "The data were collected by the Centre for Inventions and Scientific Information (\"CISI\") and consist of text data about 1,460 documents and 112 associated queries. Its purpose is to be used to build models of information retrieval where a given query will return a list of document IDs relevant to the query. The file \"CISI.REL\" contains the correct list (ie. \"gold standard\" or \"ground proof\") of query-document matching and your model can be compared against this \"gold standard\" to see how it has performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39afaee0-05b2-4e67-b032-68d50e4780b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp_retrieval_dataset/CISI.QRY\n",
      "nlp_retrieval_dataset/CISI.ALL\n",
      "nlp_retrieval_dataset/CISI.REL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "for dirname, _, filenames in os.walk('nlp_retrieval_dataset/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea88bc6-96ee-4a02-8e38-f23063840e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2162ecf8-baa8-4c11-b470-bd738c2e2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_documents(file_path: str) -> Dict[int, str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = \"\"\n",
    "        for line in file:\n",
    "            lines += \"\\n\" + line.strip() if line.startswith(\".\") else \" \" + line.strip()\n",
    "        lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "\n",
    "    documents = {}\n",
    "    doc_id = 0\n",
    "    doc_text = \"\"\n",
    "    for line in lines:\n",
    "        if line.startswith(\".I\"):\n",
    "            if doc_id != 0:\n",
    "                documents[doc_id] = doc_text.lstrip(\" \")\n",
    "            doc_id = int(line.split(\" \")[1].strip())\n",
    "            doc_text = \"\"\n",
    "        elif line.startswith(\".X\"):\n",
    "            if doc_id != 0:\n",
    "                documents[doc_id] = doc_text.lstrip(\" \")\n",
    "                doc_id = 0\n",
    "        else:\n",
    "            doc_text += line[3:].strip() + \" \"  # Ignore the first 3 characters of each line.\n",
    "\n",
    "    if doc_id != 0:\n",
    "        documents[doc_id] = doc_text.lstrip(\" \")\n",
    "\n",
    "    return documents\n",
    "\n",
    "def parse_queries(file_path: str) -> Dict[int, str]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = \"\"\n",
    "        for line in file:\n",
    "            lines += \"\\n\" + line.strip() if line.startswith(\".\") else \" \" + line.strip()\n",
    "        lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "\n",
    "    queries = {}\n",
    "    qry_id = 0\n",
    "    for line in lines:\n",
    "        if line.startswith(\".I\"):\n",
    "            qry_id = int(line.split(\" \")[1].strip())\n",
    "        elif line.startswith(\".W\") and qry_id != 0:\n",
    "            queries[qry_id] = line[3:].strip()  # The actual query text follows \".W\".\n",
    "            qry_id = 0\n",
    "\n",
    "    return queries\n",
    "\n",
    "def parse_relevance(file_path: str) -> Dict[int, List[int]]:\n",
    "    relevance = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            qry_id = int(line.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0])\n",
    "            doc_id = int(line.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])\n",
    "            if qry_id in relevance:\n",
    "                relevance[qry_id].append(doc_id)\n",
    "            else:\n",
    "                relevance[qry_id] = []                    \n",
    "                relevance[qry_id].append(doc_id)\n",
    "\n",
    "    return relevance\n",
    "\n",
    "def read_data(all_file_path: str, query_file_path: str, rel_file_path: str) -> Tuple[Dict[int, str], Dict[int, str], Dict[int, List[int]]]:\n",
    "    doc_set = parse_documents(all_file_path)\n",
    "    qry_set = parse_queries(query_file_path)\n",
    "    rel_set = parse_relevance(rel_file_path)\n",
    "\n",
    "    return doc_set, qry_set, rel_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe11bcb5-edb5-4a46-b070-343cc9a9f365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 112, 76)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, q, r = read_data(\n",
    "    'nlp_retrieval_dataset/CISI.ALL',\n",
    "    'nlp_retrieval_dataset/CISI.QRY',\n",
    "    'nlp_retrieval_dataset/CISI.REL'\n",
    ")\n",
    "\n",
    "len(d), len(q), len(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e117c-1ad3-48bb-b8f8-c19ee31f0278",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfcd276-b27a-4769-ac07-60c6da532d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "document = \n",
      "\t'18 Editions of the Dewey Decimal Classifications Comaromi, J.P. The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad. ' \n",
      "\n",
      "query =\n",
      "\t'What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?' \n",
      "\n",
      "relation=\n",
      "\t'[28, 35, 38, 42, 43, 52, 65, 76, 86, 150, 189, 192, 193, 195, 215, 269, 291, 320, 429, 465, 466, 482, 483, 510, 524, 541, 576, 582, 589, 603, 650, 680, 711, 722, 726, 783, 813, 820, 868, 869, 894, 1162, 1164, 1195, 1196, 1281]'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\ndocument = \\n\\t'{d[1]}' \\n\\nquery =\\n\\t'{q[1]}' \\n\\nrelation=\\n\\t'{r[1]}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69aa72c3-1372-414d-8b9a-a449f2103e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_k(q, d, r, qidx, k=3):\n",
    "    print(\"*\"*30)\n",
    "    print(f\"QUERY {qidx}: {q[qidx]}\")\n",
    "    print(\"*\"*30)\n",
    "    for i in range(k):\n",
    "        print(f\"DOC {i}:\\n{d[r[qidx][i]]}\")\n",
    "        print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d65511b0-729e-482a-b97c-ef2c79ba33c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "QUERY 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "******************************\n",
      "DOC 0:\n",
      "A Note on the Pseudo-Mathematics of Relevance Taube, M. Recently a number of articles, books, and reports dealing with information systems, i.e., document retrieval systems, have advanced the doctrine that such systems are to be evaluated in terms of the degree or percentage of relevancy they provide. Although there seems to be little agreement on what relevance means, and some doubt that it is quantifiable, there is, nevertheless, a growing agreement that a fixed and formal relationship exists between the relevance and the recall performance of any system.  Thus, we will find in the literature both a frankly subjective notion of relevance as reported by individual users, and equations, curves, and mathematical formulations which presumably provide numerical measures of the recall and relevance characteristics of information systems.  This phenomenon of shifting back and forth from an admittedly subjective and non-mathematical term to equations in which the same term is given a mathematical value or a mathematical definition has its ancient parallel in discussions of probability.  One cannot, of course, legislate the meaning of a term.  It all depends, as Alice pointed out, on \"who is master,\" the user or the term.  On the other hand, the use of a single term in the same document to cover two or more distinct meanings, especially when such a usage is designed to secure the acceptance of a doctrine by attributing to it mathematical validity which it does not have, represents a more serious situation than merely careless ambiguity. \n",
      "******************************\n",
      "DOC 1:\n",
      "Comparisons of Four Types of Lexical Indicators of Content Rath, G.J. Resnick, A. Savage, T.R. An experiment was conducted to determine which of four types of lexical indicators of content could be utilized best by subjects to determine relevant from irrelevant documents and to answer a set of 100 questions.  The results indicate that there were no major differences between the groups using complete text and abstracts to select relevant documents, but the group utilizing the complete text obtained a significantly higher score on the examination. \n",
      "******************************\n",
      "DOC 2:\n",
      "Machinelike Indexing by People Montgomery, C. Swanson, D.R. A study of several thousand entries in a classified bibliography of article titles (the Index Medicus) revealed that a large proportion of the title entries contained words identical to or synonymous with words of the corresponding subject heading.  It is inferred that a major part of the bibliography studied could have been compiled by a machine procedure operating on titles alone, provided the machine was supplied with a suitable synonym dictionary. \n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "sample = show_top_k(q, d, r, 1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dbda4386-dae9-42ce-8a39-73bb82995b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_single_article_into_messages(q, d, r, qidx):\n",
    "    system_prompt = f\"\"\"\n",
    "Please use the article below to answer the question. Don't mention the article title and please be brief.\n",
    "\n",
    "--- Begin article ---\n",
    "{d[r[qidx][0]]}\n",
    "--- End article ---\n",
    "\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{q[qidx]}\"},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def format_all_articles_into_messages(q, d, r, qidx, n: int=None):\n",
    "    if n:\n",
    "        relevant_docs = '\\n\\n'.join(f\"Article {i}: {d[qid]}\" for i, qid in enumerate(r[qidx]) if (i+1) <= n_articles)\n",
    "    else:\n",
    "        # Swap i for qid if you want the actual id, i is just a counter\n",
    "        relevant_docs = '\\n\\n'.join(f\"Article {i}: {d[qid]}\" for i, qid in enumerate(r[qidx]))\n",
    "    system_prompt = f\"\"\"\n",
    "Please use the article(s) below to answer the question. Don't mention the article title and please be brief.\n",
    "\n",
    "--- Begin articles ---\n",
    "{relevant_docs}\n",
    "--- End articles ---\n",
    "\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{q[qidx]}\"},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6a943596-10df-4934-8f91-09864d1524ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n",
      "\n",
      " \n",
      "Please use the article(s) below to answer the question. Don't mention the article title and please be brief.\n",
      "\n",
      "--- Begin articles ---\n",
      "Article 0: A Note on the Pseudo-Mathematics of Relevance Taube, M. Recently a number of articles, books, and reports dealing with information systems, i.e., document retrieval systems, have advanced the doctrine that such systems are to be evaluated in terms of the degree or percentage of relevancy they provide. Although there seems to be little agreement on what relevance means, and some doubt that it is quantifiable, there is, nevertheless, a growing agreement that a fixed and formal relationship exists between the relevance and the recall performance of any system.  Thus, we will find in the literature both a frankly subjective notion of relevance as reported by individual users, and equations, curves, and mathematical formulations which presumably provide numerical measures of the recall and relevance characteristics of information systems.  This phenomenon of shifting back and forth from an admittedly subjective and non-mathematical term to equations in which the same term is given a mathematical value or a mathematical definition has its ancient parallel in discussions of probability.  One cannot, of course, legislate the meaning of a term.  It all depends, as Alice pointed out, on \"who is master,\" the user or the term.  On the other hand, the use of a single term in the same document to cover two or more distinct meanings, especially when such a usage is designed to secure the acceptance of a doctrine by attributing to it mathematical validity which it does not have, represents a more serious situation than merely careless ambiguity. \n",
      "\n",
      "Article 1: Comparisons of Four Types of Lexical Indicators of Content Rath, G.J. Resnick, A. Savage, T.R. An experiment was conducted to determine which of four types of lexical indicators of content could be utilized best by subjects to determine relevant from irrelevant documents and to answer a set of 100 questions.  The results indicate that there were no major differences between the groups using complete text and abstracts to select relevant documents, but the group utilizing the complete text obtained a significantly higher score on the examination. \n",
      "\n",
      "Article 2: Machinelike Indexing by People Montgomery, C. Swanson, D.R. A study of several thousand entries in a classified bibliography of article titles (the Index Medicus) revealed that a large proportion of the title entries contained words identical to or synonymous with words of the corresponding subject heading.  It is inferred that a major part of the bibliography studied could have been compiled by a machine procedure operating on titles alone, provided the machine was supplied with a suitable synonym dictionary. \n",
      "\n",
      "Article 3: The Notion of Relevance Hillman, Donald J. Analysis of the problems of defining the mutual relevancies of queries and document-collections indicates that they essentially involve the problem of conceptual relatedness.. In order to consider the later problem, the question of concept-formation is first discussed, which in turn requires a definition of concept.. An extensional interpretation is suggested whereby a concept is associated with a class of conceptually-similar documents.. Users' similarity- judgments then provide the empirical data for formal definitions of concept and conceptual relatedness.. It is found, however, that certain very general difficulties rule out the possibility of defining concepts and their relatedness by the method proposed.. Since this method is based on human relevance-judgments, it seems a natural one to adopt, so that its collapse has serious practical and theoretical consequences.. An alternative approach is therefore proposed whose elaboration will form Part II of this article.. \n",
      "\n",
      "Article 4: The Consistency of Human Judgments of Relevance Resnick, A. Savage, T.R. A comparison of the ability of humans to consistently judge the relevance of documents to their general interests from bases of citations, abstracts, keywords, and total text was made under controlled experimental conditions.. The results showed that 1) humans are able to make such judgments consistently, and 2) the consistency of the judgment is independent of the particular base from which it is made.. Apparent inconsistency arising from judgments made on the basis of abstracts remains unexplained.. This experiment, as well as others concerned with human evaluations of text material, leave unexplored the basic problem of providing a metric scale on which such evaluations can be measured.. \n",
      "\n",
      "Article 5: Comparative Indexing: Terms Supplied by Biomedical Authors and by Document Titles Schultz, Claire K. Schultz, Wallace L. Orr, Richard H. The original aim of this study was to obtain objective data bearing on the much argued question of whether author indexing is \"good\".. Author indexing of 285 documents reporting biomedical research was scored by comparing the author- supplied terms (author set) for each paper with a criterion set of terms that was  established by asking a group of 12 potential users to describe the same document.. Terms in the document title (title set) were scored similarly.. The average author set contained almost half of all the terms employed by more than one member of the user group and scored 73% of the maximal possible score, as compared with 44% for the average title set.. When judged by the method and criterion employed here, author indexing is substantially better than indexing derived from document titles.. The findings suggest that indicia supplied by an author should serve scientists in biomedical disciplines other than his own about as well as they serve his disciplinary colleagues.. The general method developed for measuring indexing quality may represent a practical yardstick of wide applicability.. \n",
      "\n",
      "Article 6: A Study of Searching the Eye Research Literature Miller, Russell R. The paper is a report of most of the major findings of a study in searching the periodical eye research literature.. Questions were collected from eye researchers and a selected group of these were searched in nine different secondary sources.. Articles thought to be relevant were Xeroxed and sent to the eye researchers who subsequently rated the articles.. Articles of eye research interest are found in a wide variety of journals, but a small number of journals carry a large proportion of the articles judged valuable by the eye researchers.. Approximately a fourth of eye research articles are in foreign languages.. Translations are not readily available.. Despite a delay of more than 15 month between the original appearance of article in journals and the mailing of photocopies, about half of the articles of interest to the researchers were not known to them previously.. For extensive retrospective searches more than one secondary service must be used.. Index Medicus and Excerpta Medica (Section 12) or Ophthalmic Literature would be good sources.. MEDLARS demand searches were not shown to be clearly superior to manual searches of Index Medicus.. Titles, abstracts, and full text were shown to be equally effective in permitting searches to retrieve references that were subsequently rated as relevant by the researchers.. A searcher with a background in ophthalmology was able to retrieve more articles of research interest than other non-ophthalmologist searchers.. \n",
      "\n",
      "Article 7: Biomedical Literature: Analysis of Journal Articles Collected by a Radiation- and Cell-Biologist Leith, John D. Jr. The author's journal reference cards for 1965 and 1966 were analyzed according to three \"interest patterns\": (I) the total collection of 1469 article titles, a \"potentially useful\" set; (II) a subset concerning only his research speciality; and (III) a subset of articles defined as \"useful\". For each pattern, journals were ranked by frequency of use and a scatter diagram was drawn.. Patterns I and II largely resembled patterns obtained by counting citations in basic journals or by counting publications of selected researchers.. Pattern III was more widely scattered..  It is concluded that access to diverse journals is needed by researchers to supply new ideas, and that this diversity of reading is not reflected adequately by citation counting or other indirect means.. Current Contents, used by the author for current-awareness purposes in building his card file, generated 88% of all articles.. Scatter diagrams indicated the decreased scatter predicted from its use.. The 30% most important journals in this collection, including about 80% of titles, are ranked for each pattern.. \n",
      "\n",
      "Article 8: A Comparison of a Keyword from Title Index with a Single Access Point per Document Alphabetic Subject Index Jahoda, G. Stursa, Mary Lou Two indexes to a collection of 3,204 documents in the field of chemistry were  test-searched.. The indexes are a keyword from title index without added keywords and a single access point per document alphabetic subject index.. The indexes were searched by 13 graduate chemistry students using 55 questions.. Search results are characterized in terms of recall, precision, and search time.. There is no statistically significant difference in recall and precision search result between the multiple access points per document keyword from title index and the single access point per document alphabetic subject index.. Search time was significantly better for the alphabetic subject index for all but those questions having only one relevant document.. \n",
      "\n",
      "Article 9: Current Awareness Searches on CT, CBAS and ASCA Abbot, M.T.J. Hunter, P.S. Simkins, M.A. During the past year we have been one of the organizations participating in the Chemical Society's experiment on the use of routine computer searches of Chemical Titles (CT) and Chemical- Biological Activities (CBAC) for current awareness.  For some time we have also been subscribing to the Automatic Subject Citation Alert (ASCA), which is produced by the Institute for Scientific Information as a by-product of the Science Citation Index.  These three sources differ in their scope and methods, but share the same ultimate objective of providing a computer-based current awareness service. CT covers journals in all branches of chemistry, but provides only authors and titles, the latter translated into American and edited by breaking down complex words so that word fragments can be retrieved. CBAC covers only papers on the interaction of chemical compounds with biological systems, but provides abstracts which are available for computer search.  The computer can also search for molecular formulae and for Chemical Abstracts registry numbers of all compounds included in the abstracts.  ASCA in its original form was based on citations: the search profile can consist of a list of references to older work, and the output is then a list of new papers citing this work.  Last spring ASCA introduced a 'term search', which is a search for words in the titles of current papers and is therefore analogous to a CT search. This paper discusses and compares the results we have obtained so far with these three services, and the potential use of systems of this type. \n",
      "\n",
      "Article 10: Selected list of Books and Journals for the Small Medical Library Brandon, Alfred N. This updated list of 410 books and 136 journals is intended as a selection aid for the small library of a hospital, medical society, clinic, or similar organization.. Books and journals are arranged by subject, with the books followed by an author index, and the journals by an alphabetical title listing.. Items suggested for first purchase by smaller libraries are noted by an asterisk.. To purchase the entire collection of books and to pay for the annual subscription costs of all the journals would require an expenditure of about 12,000.. To acquire only those items suggested for first purchase, approximately $3,250 would be needed.. \n",
      "\n",
      "Article 11: A Cooperative Serial Acquisition Program: Thoughts on a Response to Mounting Fiscal Pressures Jones, C. Lee A regionally cooperative method of distributing responsibility for every serial title in a region is outlined.. The system assures the equitable distribution of the number of titles for which each library is committed.. Later refinements suggest an equalization of cost commitments on the basis of fiscal resources available for serial purchases.. It is pointed out that fiscal realities will force some sort of serial acquisition cooperation for all viable medical libraries.. \n",
      "\n",
      "Article 12: Use of Medical and Biological Journals in the Yale Medical Library Kilgour, F.G. This paper presents data for the end of 1960 on recorded use of some two hundred of the most often used scientific and medical journals in the Yale Medical Library.  The investigation was designed to identify the most abundantly used titles of recent date of publication to guide the acquisition of multiple subscriptions.  At the same time data was collected to distinguish heavily used back sets. \n",
      "\n",
      "Article 13: World Biomedical Journals, 1951-60: A Study of the Relative Significance of 1,388 Titles Indexed in Current List of Medical Literature Raisig, L. Miles This study is an application of the relationship of serial articles published to serial articles cited, developed in theory in the author's \"Statistical Bibliography in the Health Sciences\" (BULLETIN 50: 450-461, July 1962).. A ranked list of the indexes of significance of most of the serials indexed in Current List of Medical Literature was derived and erected from 21,000 citations secured in a random sampling of 1962 and 1961 biomedical journals regularly received in the Yale Medical Library.. The author measures the gross indexing effectiveness of Current List against his indexes of significance , offers his method  and results as means to reach objective standards for indexing and abstracting, and projects his results  as measures of general value of the serials analyzed.. \n",
      "\n",
      "Article 14: Selected Reference Aids for Small Medical Libraries Duncan, Howertine Farrell This annotated list of 178 items is compiled as a guide to the development of the reference collection in a small medical library.. Arrangement, following the pattern of the previous revision, is by broad subject groups.. Titles are chiefly in English.. Textbooks in subject fields have been omitted since these are covered adequately in several comprehensive guides to the literature.. \n",
      "\n",
      "Article 15: Sampling and Short-Period Usage in the Purdue Library Jain, A. K. Several possible methods of sampling of social science monograph titles in the general library of Purdue University were considered, and a \"good\" method was used to obtain estimates of their usage in the library and at home during the period July 1 - August 4, 1964.. The term relative usage was defined and used to study the effect of: (1) language, (2) country of publication, (3) year of publication, and (4) year of accession of a monograph title.. An attempt was made to fit a regression model for titles in English by quantifying the last three independent variables with relative usage as the dependent variable.. Functions based on the above variables have been developed to identify monograph titles for storage.. A questionnaire was employed to stady the usage of library facilities and to gather opinions of library patrons.. Purpose of visiting the library, reason for checkout of library material, reason for preferring library or home for the use of library material, etc., were analyzed on the basis of the replies received.. \n",
      "\n",
      "Article 16: A Generalized Methodology for Library Systems Analysis Burns, R.W. This article is directed toward the service in systems work.  Its purpose is to generalize at a very elementary level a methodology or approach which can be used in conducting a systems study. Systems work is discussed here as a point of view; a logical, coherent, from the top down, preface to decision-making and resource allocation which utilizes a very powerful body of sophisticated techniques.  The approach and techniques reviewed in this paper, however, will be those on the most elementary  level.  No attempt will be made to discuss the techniques of queueing, inventory management, linear programming, simulation, marginal analysis, game theory, statistical inference, or any of the other highly sophisticated techniques available to the operations research systems analysis (OR/SA) analyst. When the systems approach is clearly understood and properly used, it becomes a potent weapon in the arsenal of the administrator.  Rather than a review of the tools themselves, a delineation of this systems methodology and point of view will be considered in this article.  The methodology discussed here embraces a number of standard techniques used by the systems engineer, time and motion analyst, operations researcher, and occasionally, even the librarian. \n",
      "\n",
      "Article 17: The Teachable Language Comprehender: A Simulation Program and Theory of Language Quillian, M.R. The Teachable Language Comprehender (TLC) is a program designed to be capable of being taught to \"comprehend\" English text.  When text which the program has not seen before is input to it, it comprehends that text by correctly relating each (explicit or implicit) assertion of the new text to a large memory.  This memory is a \"semantic network\" representing factual assertions about the world. The program also creates copies of the parts of its memory which have been found to relate to the new text, adapting and combining these copies to represent the meaning of the new text.  By this means, the meaning of all text the program successfully comprehends is encoded into the same format as that of the memory.  In this form it can be added into the memory. Both factual assertions for the memory and the capabilities for correctly relating text to the memory's prior content are to be taught to the program as they are needed.  TLC presently contains a relatively small number of examples of such assertions and capabilities, but within the system, notations for expressing either of these are provided.  Thus the program now corresponds to a general process for comprehending language, and it provides a methodology for adding the additional information this process requires to actually comprehend text of any particular kind. The memory structure and comprehension process of TLC allow new factual assertions and capabilities for relating text to such stored assertions and capabilities for relating text to such stored assertions to generalize automatically. That is, once such an assertion or capability is put into the system, it becomes available to help comprehend a great many other sentences in the future. \n",
      "\n",
      "Article 18: The Information Content of Titles in Engineering Literature Bottle, Robert T. Since many alerting and information services rely very heavily on the use of titles to transfer information to the potential user, it is essential that he be aware of the proportion of the information contained in the complete document which will not be deducible from the title and which he will therefore miss.. Methods will be discussed for analyzing the relative information content of the titles of engineering paper and results presented for the amount and type of information lost through scanning title listing only.. Between one-third and one-half of indexable terms are not retrievable from article titles even if all possible synonyms and  related terms are used.. If all synonyms are used instead of one keyword the amount of information retrieved is increased by about 70 percent.. The problems of dealing with synonyms and with syntactical variants in searching titles indexes are discussed.. The possibility of using keywords in journal titles as supplementary retrieval tags is suggested since they were deemed useful in nearly one-third of the sample of papers analyzed.. \n",
      "\n",
      "Article 19: Using Commercially Available Literature Tapes for a Current Awareness Service Corbett, L. The paper reviews the need for current awareness services and describes the basic characteristics of SDI, indicating its advantages.. Details are given of the problems that have arisen in providing an SDI service based on Chemical Titles tapes at Aldermaston with particular reference to program limitations.. Data on operating costs and on use assessments of the service are given.. The pros and cons of title-only alerting systems are discussed.. \n",
      "\n",
      "Article 20: The University of Sheffield Biomedical Information Project Barkla, J. K. An outline is given of the history of the Project and the development, with OSTI support, of an information service in intestinal absorption which is intended to become self-supporting.. Results of an evaluation of computer-based current awareness techniques including journal scanning is discussed with reference to cost, completeness and minimum delay.. A simple technique is suggested for profile construction e.g. for Chemical Titles computer search, based on frequency and specificity of words in a sample of relevant titles.. \n",
      "\n",
      "Article 21: Some Experiments in the Selective Dissemination of Information in the Field of Plasma Physics Anthony, L. J. Cheney, A. G. Whelan, E.K. A small-scale, computer-based SDI system in plasma physics and the related subjects is described briefly.. The system serves about 100 research scientists and engineers and uses title input only in order to minimize input costs.. The implications of this approach and its effect upon the system parameters is discussed.. Some comparison of the costs of the computer-based system with those of a manual system is made.. Further experiments are described in which the service is expanded to external users on a world-wide basis, the aim being to compare, under controlled conditions, the parameters of the small-scale internal service with those of an external service on a wide scale.. The paper concludes with some observations on the future development and organization of computer-assisted services, their possibilities and the main problems which are likely to arise.. \n",
      "\n",
      "Article 22: Performance of Automatic Information Systems Lesk, Michael E. The SMART document retrieval system is used to investigate algorithms for text analysis and request searching.. Results from three document collections indicate that word normalization is efficiently performed by automatic thesaurus lookup, while phrase matching procedures, statistical association methods, and concept hierarchies are useful for special applications.. Automatic document clustering schemes and use-interactive feedback methods permit rapid searches of large collections.. Abstracts are found to be superior to titles as a base for content analysis in a document retrieval system and almost as good as complete texts.. Proper procedures for designing dictionaries and searching requests are discussed..The practicality of large scale document centers and their proper design are considered in light of these results.. \n",
      "\n",
      "Article 23: Retrieval Efficiency from Titles and the Cost of Indexing Tell, Bjorn V. By the means of the flexible machine search system three experiments have been made in order to test the retrieval efficiency of searching free text and keywords.. Base upon the relevance judgements of the users, the results indicate that titles and abstracts are good index material which can be used for machine searching without human indexing in the three fields studies.. \n",
      "\n",
      "Article 24: Analysis of the Microstructure of Titles in the INSPEC Data-Base Lynch, Michael F. Petrie, J. Howard Snell, Michael J. A high degree of constancy has been found to exist in the microstructure of titles of samples of the INSPEC data-base taken over 3-year period.. Character and diagram frequencies are shown to be relatively stable, while variable-length character-string characterizing samples separated by 3 years in time show close similarities.. \n",
      "\n",
      "Article 25: ISBD(S) and Title Main Entry for Serials Spalding, C. Sumner At the IFLA Liverpool Conference in 1971 a Joint working Group of the Committees on Cataloguing and on Serial Publications was set up to draw up an International Standard Bibliographic Description for Serials, taking the ISBD(M) as a model in so far as practicable.  As might be expected, the special problems presented by serial publications made the task of developing an ISBD(S) a difficult assignment which the Joint Working Group tackled with great energy and devotion.  The successive drafts were prepared by the Chairman and the Secretary, Mlle M.-L. Bossuat and Mlle M. Pelletier. Probably no data element presented such a severe problem as that of serial title.  The seemingly countless Mitteilungen, Memoires, Proceedings, Bulletins, Trudy's, and the like seemed to demand some useful and standardized way to be identified.  A solution to this problem was found in the adoption of a device which consisted of marrying the author statement to the generic title proper, with a wedding ring consisting of a space-hyphen- space and dubbing the happy couple the \"distinctive title.\" \n",
      "\n",
      "Article 26: Classification of Scientific Documents by Means of Self_Generated Groups Employing Free Language Feinman, R. D. Kwok, K. L. A study was undertaken to classify mechanically a document collection using the free-language words in the titles and abstracts of a corpus of 261 physics research papers.. Using a clustering algorithm, results were obtained which closely duplicated the clusters obtained by previous experiments with citations.. A brief comparison is made with a traditional manual classification system.. It is shown that the mechanical procedure is capable of achieving simultaneous average relevance and recall figures above 80%.. \n",
      "\n",
      "Article 27: Title Indexes as Alerting Services in the Chemical and Life Sciences Bottle, Robert T. The principles underlying alerting services are discussed.. General alerting services (as distinct from SDI systems) need to transfer to their users a large quantity of current but mainly irrelevent information as speedily as possible.. As title indexes are the easiest to prepare and are therefore common, the user needs to know how much significant information is not discernible from a documents title.. This is estimated to be 20-25% but can vary with subject and type of information sought.. If a search of, say, Chemical Titles is made, ignoring synonyms but allowing for all syntactical variants, only about one third of the significant information will be recovered.. Synonyms and other nomenclature problems are discussed.. Delay times and time of use are the two most important factors in evaluating an alerting service and are reviewed for some typical services.. \n",
      "\n",
      "Article 28: Are Titles of Chemical Papers Becoming More Informative? Tocatlian, Jacques J. The efficiency of key-work-in-context (KWIC) permuted-title indexes and their numerous variations is highly dependent upon authors' choices of titles for their papers.. Titles are important not only in commercial services, such as Chemical Titles, BASIC, Current Contents, and CA Condensates, but also in scanning primary journals, and in traditional library services, such as bibliographies.. It is generally believed and often stated that titles of chemical papers are becoming more informative as authors become increasingly aware of the importance of titles as \"carriers\" of information.. The present study was undertaken to test whether (1) titles of chemical papers are becoming more informative and (2) whether uninformative titles of chemical papers are being eliminated since the advent of the KWIC index in 1958.. The first hypothesis was tested by comparing titles published in 1948, 1958, and 1968 by the following criteria: (1) a count of substantive words in the title; (2) a count of all word matches between title and 10 leading substantive words selected from the abstract, with and without the use of a thesaurus; and (3) a count of word matches between title and 10 leading substantive words selected from the abstract, with and without the use of a thesaurus.. The second hypothesis was tested by comparing a count of short titles (with 3 or less substantive words) published in 1948, 1958, and 1968.. Results confirm that uninformative titles of chemical papers are being eliminated and that informative titles are becoming more informative since the advent of the KWIC index.. \n",
      "\n",
      "Article 29: The Efficiency of MEDLARS Titles for Retrieval Miller, William L. Previous research has indicated that the titles rather than index terms would, in the standard MEDLARS system, gave lower Recall but higher Precision.. A title searching technique is described which allows the number of references retrieved to be fixed before a search commences.. With this technique the greater applicability of title-terms offsets their relative paucity.. The title-searching technique is tested using queries put to MEDLARS.. These queries were not specially solicited for the test.. Title searching is compared with the standard MEDLARS index term search and with an index term search with fixed output size.. For equal output sizes, Title searching retrieves 4 relevant references for every 5 retrieved by index term searching.. Thus the relative retrieval efficiency of Title and Index terms is so close that the choice of one method or the other must be primarily on economic grounds.. \n",
      "\n",
      "Article 30: Relative Effectiveness of Titles, Abstracts, and Subject Headings for Machine Retrieval from the COMPENDEX Services Byrne, Jerry R. We have investigated the relative merits of searching on titles, subject headings, abstracts, free-language terms, and combinations of these elements.. The COMPENDEX data base  was used for this study since it contained all of the data elements of interest.. In general, the results obtained from the experiments indicate that, as expected, titles alone are not satisfactory for efficient retrieval.. The combination of titles and abstracts came the closest to 100% retrieval, with searching of abstracts alone doing almost as well.. Indexer input, although necessary for 100% retrieval in almost all cases, was found to be relatively unimportant.. \n",
      "\n",
      "Article 31: SWIFT:  Computerized Storage and Retrieval of Technical Information Ackermann, H.J. Haglind, J.B. Lindwall, H.F. Maizell, R.E. A unique method of computerized storage and retrieval of technical information is applied in the SWIFT (Significant Word in Full Title) program.  SWIFT chooses potential keywords from the titles, compares the key words with an exclusion word glossary to remove insignificant words and an internal glossary to prevent duplication of terms.  The keywords may be either a full or fragmented term. An option is available to index also by author.  Indexes, containing full citations, are printed periodically and cumulatively.  The magnetic tape file is available for computer search through a sort and print program.  Responses to inquiries conducted through the computer program are listed in full citation format. \n",
      "\n",
      "Article 32: Development and Production of Chemical Titles, a Current Awareness Index Publication Prepared with the Aid of a Computer Freeman, R.R. Dysn, G.M. The introduction of Chemical titles in 1961 marked the first publication produced almost entirely by computers and other data-processing equipment.  The success of this innovation has generated many requests for more information about it.  With this in mind, we hope to encourage other organizations to make use of this technique for dissemination of information by presenting here a history of Chemical titles' development coupled with a description of its production. \n",
      "\n",
      "Article 33: Information Transfer Limitations of Titles of Chemical Documents Bottle, R.R. Seeley, C.R. Some methods of estimating the minimum amounts of information in a document not retrievable through its title are discussed.  An analysis of the information transferred by different types of keywords is helpful in planning search strategies, e.g., 30% of chemical substances mentioned in journal articles are not discernable in their titles even when broad class names are used as synonyms.  Patents have considerably less informative titles than journal articles.  In nuclear science, report titles are also less informative than those of journal articles, but the proportion of reports with completely uninformative titles is now only 10% of the 1957 value.  Titles in chemistry are more informative than those in most other fields, but the use of alerting and other services based on titles requires a good understanding of the underlying information transfer principles. \n",
      "\n",
      "Article 34: Evaluation of the Database CA Condensates Compared with Chemical Titles Hansen, Inge Berg The performance of CA Condensates and Chemical Titles based on analysis of precision and \"relative recall CT/CC\" for a collection of 46 search profiles was studied over a period of one year.. Special emphasis was laid on the function of the keyword phrases of CC and the users' attitude towards literature categories not represented in CT.. The results are discussed in terms of the value of the systems for Danish users seen from users' and the documentalist's point of view.. \n",
      "\n",
      "Article 35: Author Versus Title: A Comparative Survey of the Accuracy of the Information Which the User Brings to the Library Catalogue Ayres, F. H. German, Janice Loukes, N. Searle, R. H. Details are given of a survey carried out in a large scientific special library on the comparative accuracy of the author and title information which the user brings to the catalogue.. The sample was restricted to requests for book material.. The results are analyzed in detail and show the title to be more accurate.. Some suggestions are made for extending this type of survey.. \n",
      "\n",
      "Article 36: Comparative Efficiency of Searching Titles, Abstracts, and Terms in a Free-Text Data Base Barcer, F. H. Veal, D. C. Wyatt, B. K. The choice of the suitable data base for providing an information service is governed by factors of coverage, performance, and cost.. The cost of the data base to subscribers is a known quantity, and the coverage is decided by the data base producers.. This paper describes an investigation into the relative performance of the four major Chemical Abstracts Service magnetic tape data-base, Chemical Titles (CT), which contains the titles of citations only, Chemical Abstracts Condensates (CAC), which contains titles enriched with keyword phrases, Chemical-Biological Activities (CBAC),and Polymer Science and Technology (POST), both of which contain full digests in addition to titles.. The performance was measured in terms of the relative currency of the four data-bases, on the retrieval efficiency of profiles searched against them.. Fifty questions from industrial and government research organizations were used in the experiment.. Search profiles corresponding to these questions were constructed for searching against each database, output was assessed for relevance by users, and profile performance figures (precision and recall ratios) were calculated for each profile.. The overall retrieval efficiency of profiles searched against data-bases containing titles only, titles-plus-keywords, and titles-plus-digests, was calculated, and these results are presented.. \n",
      "\n",
      "Article 37: Studies to Compare Retrieval Using Titles with that Using Index Terms. SDI from 'Nuclear Science Abstracts' Olive, G. Terry, J. E. Datta, S. A Selective dissemination of Information service based on computer scanning of Nuclear Science Abstracts tapes has operated at Harwell since October 1968.. Users' interest profiles are constructed using Euratom index terms and NSA subject categories assigned to each item in NSA.. The performance of the mechanized SDI service has been compared with that of the pre-existing current awareness service which is based on visual scanning of journals and reports by information staff.. The visual service was found to be providing an important service of good currency and high precision, about 85%, to a limited number of users.. the mechanized service is less selective and of lower precision, approximately 50%, but can be expanded more readily.. In order to compare the effectiveness of Euratom index terms and words on titles for computer SDI matching, an experiment was set up in which sixty users of the mechanized service assessed NSA document notifications which were generated by matching either index terms and subject categories, or words in titles and subject categories, without being aware of the method of matching.. Over 10,000 document assessments, fron six issues of NSA were returned.. The average precision was 45.6% for index terms and 47.3% for title words.. Index terms retrieved more documents, in the ratio 1.13:1, but both systems missed many relevant documents retrieved by the other.. Index terms retrieved only 58% of the relevant documents selected by titles.. The converse ratio was 51%.. No significant effects of document types or subject on the relative effectiveness of two matching systems were found, but when the results were analyzed by title length it appeared that for titles longer than about 100 characters title words gave recall equal to that of index terms, though with a lower precision.. A detailed study of samples of items found by visual scanning but missed by computer matching or found by one computer method but not by the other, was made to identify reasons for failure.. \n",
      "\n",
      "Article 38: Title-Only Entries Retrieved by Use of Truncated Search Keys Kilgour, Frederick G. Long, Philip L. Liederman, Eugene B. Landgraf, Alan L. An experiment testing utility of truncated search keys as inquiry terms in an on-line system was performed on a file of 16,792 title-only bibliographic entries.. Use of a 3,3 key yields eight or fewer entries 99.0% of the time.. \n",
      "\n",
      "Article 39: A Truncated Search Key Title Index Long, Philip L. Kilgour, Frederick G. An experiment showing that 3, 1, 1, 1 search keys derived from titles are sufficiently specific to be an efficient computerized, interactive index to a file of 135,938 MARC II records.. \n",
      "\n",
      "Article 40: An Experiment in Index Term Frequency Svenonius, Elaine This paper presents an experimental study of index-term frequency as a factor in retrieval performance.. The frequency of an index term, or its \"breadth\" as it is called here, is the number of postings made to the term in a given collection.. The question is asked: Of index terms assigned to documents, which function most effectively in retrieval, the most term or popular terms, or those which are used relatively infrequently? The experiment is a retrieval experiment and uses the Cranfield-Salton data.. Breadth of indexing is varied by nonrandomly deleting terms from documents.. Retrieval output is evaluated using the Expected Search Length measure of retrieval effectiveness as well as the usual precision and recall.. The Wilcoxen Test is used to determine the statistical significance of the different indexings.. The results show that the \"optimal\" breadth of indexing is a variable, depending on user needs: if a few documents are wanted or high precision is desired, then narrow terms are more effective than broad ones; if, on the other hand, all or most relevant documents are wanted, then broad terms are better.. An argument, however, can be made for the quality of narrow terms, since when these terms are deleted precision never improves, whereas deleting broad terms always results in a higher precision.. A corollary experiment is carried out to compare two indexings of the same average breadth where one indexing consists of semantically appropriate terms - terms taken from the document title - and the other consists of merely \"reasonable\" index terms.. The result suggest that title-term indexing is qualifiedly superior.. \n",
      "\n",
      "Article 41: On basic features of information retrieval language for information retrieval by title. Part 1 Stokolova, N.A. Vleduts, F.E. Presents the basic features of variants of an informational language designed for searching titles of publications in the field of synthetic organic chemistry. The classification of terms from natural language and the specifics of translating them into information language are discussed. A method for selecting the synthetic means of informational languages is developed, and the criterion for semantic correspondence and search algorithm is briefly described. Experiments which were conducted with 3 variants of the language developed are discussed. Conclusions are drawn on the benefits of the languages for searching, recommendations are made regarding their field of application. \n",
      "\n",
      "Article 42: On basic features of information retrieval language for information retrieval by title. Stokolova, N.H. Veeduts, F.E. Presents the basic features of variants of an informational language designed for searching titles of publications in the field of synthetic organic chemistry. The classification of terms from natural language and the specifics of translating them into information language are discussed. A method for selecting the synthetic means of informational languages is developed, and the criterion for semantic correspondence and search algorithm is briefly described. Experiments which were conducted with 3 variants of the language developed are discussed. Conclusions are drawn on the benefits of the languages for searching, recommendations are made regarding their field of application. \n",
      "\n",
      "Article 43: Comparative Effects of Titles, Abstracts and Full Texts on Relevance Judgements Saracevic, Tefko Twenty-two users submitted 99 questions to experimental IR systems and received 1086 documents as answers, receiving first titles, then abstracts, and finally full texts.. Ability of users to recognize relevance from shorter formats in comparison to full text judgement was observed.. Of 1086 answers evaluated, 843 or 78% had the same judgement on all three formats.. Of 207 answers judged relevant from full text, 131 were judged so from titles and 160 from abstracts.. Parallels between users' and IR systems' performance on shorter formats are drawn.. \n",
      "\n",
      "Article 44: Retrieval of Bibliographic Entries from a Name-Title Catalog by Use of Truncated Search Keys Kilgour, Frederick G. Long, Philip L. Leiderman, Eugene B. An experiment to produce information on the utility of co-ordinating derived, truncated search keys as enquiry terms to an on-line bibliographic system was performed on a file of 132,808 name-title entries.. Statistics on the number of entries associated with each key for keys varying from four to eight characters in length were obtained.. Assuming use of a keyboard cathode ray tube terminal capable of displaying at least ten lines of text, and taking spelling error probabilities into account, a derived key consisting of the first three characters of author name concatenate with the first three characters of title was determined to be effective for at least four-fifths of all academic libraries.. \n",
      "\n",
      "Article 45: Relative Effectiveness of Document Titles and Abstracts for Determining Relevance of Documents Resnick, A. Abstract.  Individuals who received documents through a selective dissemination of information system were asked to determine the relevance of documents to their work interests on the basis of titles and of abstracts.  The results indicate that there was no significant difference between the usefulness of titles and of abstracts for this purpose. \n",
      "--- End articles ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_message = format_all_articles_into_messages(q, d, r, 1)\n",
    "print(sample_message[1]['content'], \"\\n\\n\",sample_message[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fc6ff9c2-5c8f-49a2-972b-f3d4c7cb8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles? \n",
      "\n",
      " \n",
      "Please use the article below to answer the question. Don't mention the article title and please be brief.\n",
      "\n",
      "--- Begin article ---\n",
      "A Note on the Pseudo-Mathematics of Relevance Taube, M. Recently a number of articles, books, and reports dealing with information systems, i.e., document retrieval systems, have advanced the doctrine that such systems are to be evaluated in terms of the degree or percentage of relevancy they provide. Although there seems to be little agreement on what relevance means, and some doubt that it is quantifiable, there is, nevertheless, a growing agreement that a fixed and formal relationship exists between the relevance and the recall performance of any system.  Thus, we will find in the literature both a frankly subjective notion of relevance as reported by individual users, and equations, curves, and mathematical formulations which presumably provide numerical measures of the recall and relevance characteristics of information systems.  This phenomenon of shifting back and forth from an admittedly subjective and non-mathematical term to equations in which the same term is given a mathematical value or a mathematical definition has its ancient parallel in discussions of probability.  One cannot, of course, legislate the meaning of a term.  It all depends, as Alice pointed out, on \"who is master,\" the user or the term.  On the other hand, the use of a single term in the same document to cover two or more distinct meanings, especially when such a usage is designed to secure the acceptance of a doctrine by attributing to it mathematical validity which it does not have, represents a more serious situation than merely careless ambiguity. \n",
      "--- End article ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_message = format_single_article_into_messages(q, d, r, 1)\n",
    "\n",
    "print(sample_message[1]['content'], \"\\n\\n\",sample_message[0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf738a-d87f-4080-9531-2b05e1eacfbd",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "1. For every query we have a set of related articles\n",
    "   - For every article, we will generate an answer to the query based on the question to give us a generated answer.\n",
    "   - Alternatively, we can inject all articles in the context and get the answer\n",
    "2. For aritcles not in the related set, we can make the response \"N/A\" to include them for training.\n",
    "3. Then we format the data based on the instruction format in SFTTrainer.\n",
    "4. Then we can fine tune it.\n",
    "\n",
    "We will do (1a)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d0566-7578-4f90-a2ce-f05038d676ce",
   "metadata": {},
   "source": [
    "# Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bff33933-ca0d-44b2-b23b-477580aeb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_data = {}\n",
    "for query_id in q:\n",
    "    try:\n",
    "        message_data[query_id] = {\"messages\": format_all_articles_into_messages(q, d, r, query_id)}\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4f8449cf-1397-4cc7-8158-cda3c74d6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_id in message_data:\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", messages=message_data[query_id]['messages']\n",
    "        )\n",
    "        message_data[query_id]['answer'] = completion.choices[0].message.content\n",
    "    except:\n",
    "        print(f\"query {query_id} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "30ada8ba-5790-4a5e-ab67-441bada1a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"labeled_CISI_IR_data.pickle\", 'wb')\n",
    "pickle.dump(message_data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2cb6bb91-5fe5-4fe8-b890-540647ba8940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Problems in creating descriptive titles include the challenge of adequately capturing the essence and specifics of an article while maintaining brevity and clarity. Titles must balance informativeness and conciseness, often leading to difficulty in conveying enough information without being overly complex or vague.\\n\\nAutomatically retrieving articles based on approximate titles faces challenges such as synonyms, variations in terminology, and informal language. Over-reliance on titles may miss significant content not reflected in the title, leading to inadequate or irrelevant search results.\\n\\nGenerally, the relevance of article content to their titles can vary widely. While well-crafted titles can effectively encapsulate key aspects of an article, many titles may fail to convey all significant information, as studies indicate that a substantial portion of relevant content may not be retrievable from titles alone.'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_data[1]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc969d0-17b5-42fc-bad0-cf6139f08c98",
   "metadata": {},
   "source": [
    "# Testing an example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f2cc4-650c-4d7f-8ac1-69974c1c26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95bc006b-8dc3-4b25-88aa-c2bb64696dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "711307b8-98c8-47e7-8f86-5b5c5ad97147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc44e821-2c26-491e-9023-8c539067591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57f3e064-247a-4560-9959-5867423a877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9904bfe5-a0bd-432e-a747-52d6f795eb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.Int64Scalar: 4>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['train']['label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb859437-5843-428e-a029-5b09f8a90183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.StringScalar: \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\">"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1bdd54e-1f30-4d52-845a-93b76f5c6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farceo/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a71eb7f2-7a2c-4de9-83d7-b35522fc017b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7e925da-f6cf-47a1-881c-2a17fb5e958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4129421-f535-4a35-a14e-d033d4c81b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c4adc7f-255c-42dd-8241-fbf15fd18408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a6eff65-c751-46a1-aba8-9c7b52f3c484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225c031df5f24b32b72643faa5ecc662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=120)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81be7c6b-c4df-42cd-8ae3-29467df4ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "506992f3-75a1-4bbf-83a0-3bb409f637bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04f75f0c-abec-413c-8e2f-4128555bae5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.1056952311197916, metrics={'train_runtime': 109.5025, 'train_samples_per_second': 27.397, 'train_steps_per_second': 3.425, 'total_flos': 185004943920000.0, 'train_loss': 1.1056952311197916, 'epoch': 3.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394574b-e774-4eed-b6fe-7621ae877b10",
   "metadata": {},
   "source": [
    "# Now building on the real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c13649-8317-41bc-89f4-8e7e01b795fb",
   "metadata": {},
   "source": [
    "# Some important points:\n",
    "1. not classification, instead will be more like summarization\n",
    "2. can generate yes or no questions using synthetic data generation\n",
    "3. can then tune on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b916f08-532b-4c5b-892b-7b6424366db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "QUERY 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "******************************\n",
      "DOC 0:\n",
      "A Note on the Pseudo-Mathematics of Relevance Taube, M. Recently a number of articles, books, and reports dealing with information systems, i.e., document retrieval systems, have advanced the doctrine that such systems are to be evaluated in terms of the degree or percentage of relevancy they provide. Although there seems to be little agreement on what relevance means, and some doubt that it is quantifiable, there is, nevertheless, a growing agreement that a fixed and formal relationship exists between the relevance and the recall performance of any system.  Thus, we will find in the literature both a frankly subjective notion of relevance as reported by individual users, and equations, curves, and mathematical formulations which presumably provide numerical measures of the recall and relevance characteristics of information systems.  This phenomenon of shifting back and forth from an admittedly subjective and non-mathematical term to equations in which the same term is given a mathematical value or a mathematical definition has its ancient parallel in discussions of probability.  One cannot, of course, legislate the meaning of a term.  It all depends, as Alice pointed out, on \"who is master,\" the user or the term.  On the other hand, the use of a single term in the same document to cover two or more distinct meanings, especially when such a usage is designed to secure the acceptance of a doctrine by attributing to it mathematical validity which it does not have, represents a more serious situation than merely careless ambiguity. \n",
      "******************************\n",
      "DOC 1:\n",
      "Comparisons of Four Types of Lexical Indicators of Content Rath, G.J. Resnick, A. Savage, T.R. An experiment was conducted to determine which of four types of lexical indicators of content could be utilized best by subjects to determine relevant from irrelevant documents and to answer a set of 100 questions.  The results indicate that there were no major differences between the groups using complete text and abstracts to select relevant documents, but the group utilizing the complete text obtained a significantly higher score on the examination. \n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "show_top_k(q, d, r, qidx, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1dfbc2b-8809-4676-8d79-286cfebef41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/farceo/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/transformers/training_args.py:2179: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of 🤗 Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", use_mps_device=True)\n",
    "\n",
    "TOKENIZER = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MODEL = \"BEE-spoke-data/smol_llama-101M-GQA-python\"\n",
    "\n",
    "# MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# MODEL = \"BEE-spoke-data/smol_llama-101M-GQA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd2332-a29a-4fd8-b678-b434c950f5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f82317-5819-4641-a5ab-7909106bae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[\n",
    "        0\n",
    "    ]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "\n",
    "def run_model(sentences, tokenizer, model):\n",
    "    encoded_input = tokenizer(\n",
    "        sentences, \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\",\n",
    "        max_length=250,\n",
    "    )\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a51f1d-1bd0-42b4-983f-72d4e2e9bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL,\n",
    "    use_fast=False,\n",
    "    # use_fast=True,\n",
    ")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36449596-37d7-480c-98ce-4dcca18fb1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(d, orient='index', columns=['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb666b0-4259-47bc-b23c-ef9d67452a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape = \u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(sentences, tokenizer, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute token embeddings\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m mean_pooling(model_output, encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     26\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(sentence_embeddings, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:950\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    947\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     use_cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, Cache) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    955\u001b[0m ):  \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/Python/demos/kfp-demos/venv/lib/python3.10/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "embeddings = run_model(df[\"query\"].tolist(), tokenizer, model)\n",
    "print(embeddings)\n",
    "print(\"shape = \", df.shape)\n",
    "df[\"Embeddings\"] = list(embeddings.detach().cpu().numpy())\n",
    "print(\"embeddings generated...\")\n",
    "# df[\"event_timestamp\"] = pd.to_datetime(\"today\")\n",
    "df[\"document_id\"] = df.index\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44387a-5bc1-4253-a4c5-3fed57f9d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER)\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "query_embedding = run_model(question, tokenizer, model)\n",
    "query = query_embedding.detach().cpu().numpy().tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301be40-f1c8-41ef-9df1-1e0a3bb4ee28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70737147-e4e9-4a96-9910-dcd440760976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# text is converted to lowercase and split into words\n",
    "def get_words (text):\n",
    "    word_list = [word for word in word_tokenize (text.lower ())]\n",
    "    return word_list\n",
    "    \n",
    "doc_words = {}\n",
    "qry_words = {}\n",
    "\n",
    "for doc_id in documents.keys ():\n",
    "    doc_words [doc_id] = get_words (documents.get (doc_id))\n",
    "for qry_id in queries.keys ():\n",
    "    # entries in both documents and queries are represented as word lists\n",
    "    qry_words [qry_id] = get_words (queries.get (qry_id))\n",
    "    \n",
    "# print out the length of the dictionaries and check the first document and the fisrt query\n",
    "print(len (doc_words))\n",
    "print(doc_words.get (\"1\"))\n",
    "print(len (doc_words.get (\"1\")))\n",
    "print(len (qry_words))\n",
    "print(qry_words.get (\"1\"))\n",
    "print(len (qry_words.get(\"1\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
